{
  "metadata": {
    "input_documents": [
      "doc1.pdf",
      "doc2.pdf"
    ],
    "persona": "Graduate Student studying Chemistry and Machine Learning",
    "job_to_be_done": "Identify and extract important sections that explain the fundamentals of reaction kinetics and provide an overview of attention-based deep learning models, so I can understand how machine learning can be applied to chemical simulations"
  },
  "extracted_sections": [
    {
      "document": "doc1.pdf",
      "page": 2,
      "section_title": "2 \n1",
      "importance_rank": 1,
      "refined_text": "2 \n1.  Introduction \n \nChemical reaction kinetics deals with the rates of chemical processes.   Any chemical process may \nbe broken down into a sequence of one or more single-step processes known either as elementary \nprocesses, elementary reactions, or elementary steps.  Elementary reactions usually involve either \na single reactive collision between two molecules, which we refer to as a a bimolecular step, or \ndissociation/isomerisation of a single reactant molecule, which we refer to as a unimolecular step.  \nVery rarely, under conditions of extremely high pressure, a termolecular step may occur, which \ninvolves simultaneous collision of three reactant molecules.  An important point to recognise is that \nmany reactions that are written as a single reaction equation in actual fact consist of a series of \nelementary steps.  This will become extremely important as we learn more about the theory of \nchemical reaction rates. \n \nAs a general rule, elementary processes involve a transition between two atomic or molecular \nstates separated by a potential barrier.  The potential barrier constitutes the activation energy of \nthe process, and determines the rate at which it occurs.  When the barrier is low, the thermal \nenergy of the reactants will generally be high enough to surmount the barrier and move over to \nproducts, and the reaction will be fast.  However, when the barrier is high, only a few reactants will \nhave sufficient energy, and the reaction will be much slower.  The presence of a potential barrier to \nreaction is also the source of the temperature dependence of reaction rates, which we will cover in \nmore detail in Section 19. \n \nThe huge variety of chemical species, types of reaction, and the accompanying potential energy \nsurfaces involved means that the timescale over which chemical reactions occur covers many \norders of magnitude, from very slow reactions, such as iron rusting, to extremely fast reactions, \nsuch as the electron transfer processes involved in many biological systems or the combustion \nreactions occurring in flames. \n  \nA study into the kinetics of a chemical reaction is usually carried out with one or both of two main \ngoals in mind: \n \n \n1. \nAnalysis of the sequence of elementary steps giving rise to the overall reaction.  i.e. \n \n \nthe reaction mechanism. \n \n \n2. \nDetermination of the absolute rate of the reaction and/or its individual elementary \n \n \nsteps. \n \nThe aim of this course is to show you how these two goals may be achieved. \n \n \n2.  Rate of reaction \n \nWhen we talk about the rate of a chemical reaction, what we mean is the rate at which reactants \nare used up, or equivalently the rate at which products are formed.  The rate therefore has units of \nconcentration per unit time, mol dm-3 s-1 (for gas phase reactions, alternative units of concentration \nare often used, usually units of pressure \u2013 Torr, mbar or Pa).  To measure a reaction rate, we \nsimply need to monitor the concentration of one of the reactants or products as a function of time.  \nThere is one slight complication to our definition of the reaction rate so far, which is to do with the \nstochiometry of the reaction.  The stoichiometry simply refers to the number of moles of each \nreactant and product appearing in the reaction equation.  For example, the reaction equation for \nthe well-known Haber process, used industrially to produce ammonia, is: \n \n \n \n \n \n \nN2 + 3H2  \u00be  2NH3 \n \nN2 has a stochiometric coefficient of 1, H2 has a coefficient of 3, and NH3 has a coefficient of 2.  \nWe could determine the rate of this reaction in any one of three ways, by monitoring the changing"
    },
    {
      "document": "doc1.pdf",
      "page": 1,
      "section_title": "1\n \n \nReaction Kinetics \n \n \nDr Claire Vallance  \nFirst year",
      "importance_rank": 2,
      "refined_text": "1\n \n \nReaction Kinetics \n \n \nDr Claire Vallance  \nFirst year, Hilary term \n \n \nSuggested Reading  \n \n \nPhysical Chemistry, P. W. Atkins  \nReaction Kinetics, M. J. Pilling and P. W. Seakins  \nChemical Kinetics, K. J. Laidler \nModern Liquid Phase Kinetics, B. G. Cox \n \n \nCourse synopsis \n \n \n \n1.   \nIntroduction \n2.  \nRate of reaction \n3.   \nRate laws \n4.   \nThe units of the rate constant \n5.   \nIntegrated rate laws \n6.   \nHalf lives \n7.   \nDetermining the rate law from experimental data \n \n \n(i)  Isolation method \n \n \n(ii)  Differential methods \n \n \n(iii)  Integral methods \n \n \n(iv)  Half lives \n8.   \nExperimental techniques \n \n \n(i)  Techniques for mixing the reactants and initiating reaction \n \n \n(ii) Techniques for monitoring concentrations as a function of time  \n \n \n(iii) Temperature control and measurement \n9.   \nComplex reactions \n10.   \nConsecutive reactions \n11.   \nPre-equilibria \n12.   \nThe steady state approximation \n13.  \n\u2018Unimolecular\u2019 reactions \u2013 the Lindemann-Hinshelwood mechanism \n \n \n14.   \nThird order reactions \n15.   \nEnzyme reactions \u2013 the Michaelis-Menten mechanism \n16.   \nChain reactions \n17.   \nLinear chain reactions \n \n \nThe hydrogen \u2013 bromine reaction \n \n \nThe hydrogen \u2013 chlorine reaction \n \n \nThe hydrogen-iodine reaction \n \n \nComparison of the hydrogen-halogen reactions \n18.   \nExplosions and branched chain reactions \n \n \nThe hydrogen \u2013 oxygen reaction \n19.   \nTemperature dependence of reaction rates \n \n \nThe Arrhenius equation and activation energies \n \n \nOverall activation energies for complex reactions \n \n \nCatalysis \n20.   \nSimple collision theory"
    },
    {
      "document": "doc1.pdf",
      "page": 19,
      "section_title": "19\nvalues of k being larger than the values predicted by the",
      "importance_rank": 3,
      "refined_text": "19\nvalues of k being larger than the values predicted by the Lindemann-Hinshelwood mechanism, as \nshown below. \n \nIt turns out that while the general idea of a collisional activation \nprocess is correct, the true mechanism of \u2018unimolecular\u2019 reactions \nis slightly more involved.  The principal failing of the Lindemann-\nHinshelwood mechanism is that it assumes that any excited \nreactant A* will undergo unimolecular reaction to produce \nproducts.  In practice, however, excitation is generally required in \na degree of freedom that is coupled to the reaction coordinate in \nsome way e.g. vibrational excitation in a bond that breaks during \nthe reaction.  More sophisticated theories of unimolecular \nreactions have been developed which take this and other factors \ninto account, and provide much better agreement with \nexperiment.   \n \n \n14.  Third order reactions \n \nA number of reactions are found to have third order kinetics.  An example is the oxidation of NO, \nfor which the overall reaction equation and rate law are given below. \n  \n \n \n \n2NO + O2 \u2192 2NO2 \n \nd[NO2]\ndt\n = k [NO]2[O2]  \n \nOne possibility for the mechanism of this reaction would be a three-body collision (i.e. a true \ntermolecular reaction).  However, such collisions are exceedingly rare, and certainly too unlikely to \nexplain the observed rate at which this reaction proceeds.  An added complication is that the rate \nof this reaction is found to decrease with increasing temperature, an indication of a complex \nmechanism.  An alternative mechanism that leads to the same rate law is a two step process \ninvolving a pre-equilibrium. \n \n \n \n \n \n \n NO + NO \nk-1\n\u00be\nk1  (NO)2  \n \n \n \n \n \n \n \n(NO)2 + O2 \u2192\nk2 2NO2  \n \nThe overall rate is  \n \n \n \n \n\u03bd  =  1\n2\nd[NO2]\ndt\n  =  k2[(NO)2][O2]  \n \nHowever, from the pre-equilibrium, we have \n \n \n \n \n \nK  =  [(NO)2]\n[NO]2      so  \n[(NO)2]  =  K[NO]2 \nand the overall rate is \n \n \n \n \n \n \n\u03bd  =  k2K [NO]2[O2] \n \ni.e. third order, as required. \n \nA very common situation in which third order kinetics are observed are reactions in which two \nreactants combine to form a single product.  Such reactions require a so-called \u2018third body\u2019 to take \naway some of the excess energy from the reaction product.  An example is the formation of ozone \n \n \n \n \n \n \nO + O2 \u2192  O3"
    },
    {
      "document": "doc1.pdf",
      "page": 13,
      "section_title": "13\nLaser-induced fluorescence \n \nIn laser-induced fluorescen",
      "importance_rank": 4,
      "refined_text": "13\nLaser-induced fluorescence \n \nIn laser-induced fluorescence a laser is used to excite a chosen species in a reaction mixture to an \nelectronically excited state.  The excited states then emit photons to return to the ground state, and \nthe intensity of this fluorescent emission is measured.  Because the number of excited states \nproduced by the laser pulse is proportional to the number of ground state molecules present in the \nreaction mixture, the fluorescence intensity provides a measure of the concentration of the chosen \nspecies. \n \n \n(iii) Temperature control and measurement \n \nFor any reaction with a non-zero activation energy, the rate constant is dependent on temperature. \nThe temperature dependence is often modelled by the Arrhenius equation, which will be treated in \nmore detail in Section 18. \n \n \n \n \n \n k = A exp(-Ea/RT) \n \nwhere Ea is the activation energy for the reaction, and A is a constant known as the pre-\nexponential factor. \n \nThis temperature dependence means that in order to measure an accurate value for k, the \ntemperature of the reaction mixture must be maintained at a constant, known value.  If activation \nenergies are to be measured as part of the kinetic study, rate constants must be measured at a \nseries of temperatures.   The temperature is most commonly monitored using a thermocouple, due \nto its wide range of operation and potential for automation; however, standard thermometers are \nalso commonly used.  \n \nThere are numerous ways in which the temperature of a reaction mixture may be controlled.  For \nexample, reactions in the liquid phase may be carried out in a temperature-controlled thermostat, \nwhile reactions in the gas phase are usually carried out inside a stainless steel vacuum chamber, \nin which thermal equilibrium at the temperature of the chamber is maintained through collisions of \nthe gas molecules with the chamber walls.  High temperatures up to 1300 K may be obtained using \nconventional heaters.  Low temperatures may be achieved by flowing cooled liquid through the \nwalls of the reaction vessel, and very low temperatures may be reached by using cryogenic liquids \nsuch as liquid nitrogen (~77 K) or liquid helium (~4 K).  Extremely low temperatures (down to a few \nKelvin), such as those relevant to reactions in interstellar gas clouds, may be obtained by \npreparing the reactant gases in a supersonic expansion (see Section 9 of the Properties of Gases \nhandout). \n \n \n9.  Complex reactions \n \nIn kinetics, a \u2018complex reaction\u2019 simply means a reaction whose mechanism comprises more than \none elementary step.  In the previous sections we have looked at experimental methods for \nmeasuring reaction rates to provide kinetic data that may be compared with the predictions of \ntheory.  In the following sections, we will look at a range of different types of complex reactions and \nthe rate laws that may be predicted from their kinetic mechanisms.  Disagreement of a predicted \nrate law with the experimental data is enough to rule out the corresponding proposed mechanism, \nwhile agreement inspires some confidence that the proposed mechanism is the correct one.  It \nshould be noted though that agreement between the predicted and measured kinetics is not \nalways enough to assign a mechanism.  The proposed mechanism must be able to account for all \nother properties of the reaction, which may include quantities such as the product distribution, \nproduct stereochemistry, kinetic isotope effects, temperature dependence, and so on. \n \nThe types of complex mechanisms that we will cover are: consecutive (or sequential) reactions; \ncompeting reactions; pre-equlibria; unimolecular reactions; third order reactions; enzyme reactions; \nchain reactions; and explosions."
    },
    {
      "document": "doc1.pdf",
      "page": 3,
      "section_title": "3\nconcentration of N2, H2, or NH3",
      "importance_rank": 5,
      "refined_text": "3\nconcentration of N2, H2, or NH3.  Say we monitor N2, and obtain a rate of -d[N2]\ndt  = x mol dm-3 s-1.  \nSince for every mole of N2 that reacts, we lose three moles of H2, if we had monitored H2 instead of \nN2 we would have obtained a rate -d[H2]\ndt  = 3x mol dm-3 s-1.  Similarly, monitoring the concentration \nof NH3 would yield a rate of 2x mol dm-3 s-1.  Clearly, the same reaction cannot have three different \nrates, so we appear to have a problem.  The solution is actually very simple: the reaction rate is \ndefined as the rate of change of the concentration of a reactant or product divided by its \nstochiometric coefficient.  For the above reaction, the rate (usually given the symbol \u03bd)  is therefore \n \n \n \n \n \n \n\u03bd  =  -d[N2]\ndt   =  -1\n3 d[H2]\ndt   =  1\n2 d[NH3]\ndt\n    \n \nNote that a negative sign appears when we define the rate using the concentration of one of the \nreactants.  This is because the rate of change of a reactant is negative (since it is being used up in \nthe reaction), but the reaction rate needs to be a positive quantity. \n \n \n3.  Rate laws \n \nThe rate law is an expression relating the rate of a reaction to the concentrations of the chemical \nspecies present, which may include reactants, products, and catalysts.  Many reactions follow a \nsimple rate law, which takes the form \n \n \n \n \n \n \n \n\u03bd = k [A]a[B]b[C]c... \n \n \n \n(3.1) \n \ni.e. the rate is proportional to the concentrations of the reactants each raised to some power.  The \nconstant of proportionality, k, is called the rate constant. The power a particular concentration is \nraised to is the order of the reaction with respect to that reactant.  Note that the orders do not have \nto be integers.  The sum of the powers is called the overall order.  Even reactions that involve \nmultiple elementary steps often obey rate laws of this kind, though in these cases the orders will \nnot necessarily reflect the stoichiometry of the reaction equation.  For example, \n \n \n \n  \nH2 + I2 \u2192 2HI  \n \n \n\u03bd = k [H2][I2].  \n \n(3.2) \n \n \n \n \n \n3ClO\u2212  \u2192 ClO3\n\u2212 + 2Cl\u2212 \n \n\u03bd = k [ClO\u2212]2 \n \n \n(3.3) \n \nOther reactions follow complex rate laws.  These often have a much more complicated \ndependence on the chemical species present, and may also contain more than one rate constant.  \nComplex rate laws always imply a multi-step reaction mechanism. An example of a reaction with a \ncomplex rate law is  \n \n \n \nH2 + Br2 \u2192  2HBr \n \n \n\u03bd = \n[H2][Br2]1/2\n1 + k'[HBr]/[Br2]   \n(3.3) \n \nIn the above example, the reaction has order 1 with respect to [H2], but it is impossible to define \norders with respect to Br2 and HBr since there is no direct proportionality between their \nconcentrations and the reaction rate.  Consequently, it is also impossible to define an overall order \nfor this reaction.   \n \nTo give you some idea of the complexity that may underlie an overall reaction equation, a \nslightly simplified version of the sequence of elementary steps involved in the above reaction is \nshown below.  We will return to this reaction later when we look at chain reactions in Section 17. \n \n \n \n \n \nBr2   \n  \u2192 Br + Br  \n \n \n \n \nBr + H2 \u2192  H + HBr \n \n \n \n \nH + Br2 \u2192 Br + HBr \n \n \n \n \nBr + Br \u2192 Br2   \n \n \n \n \n(3.4)"
    },
    {
      "document": "doc2.pdf",
      "page": 11,
      "section_title": "[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fe",
      "importance_rank": 1,
      "refined_text": "[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\nmachine translation. CoRR, abs/1406.1078, 2014.\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\npreprint arXiv:1610.02357, 2016.\n[7] Junyoung Chung, \u00c7aglar G\u00fcl\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\nnetwork grammars. In Proc. of NAACL, 2016.\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\n[10] Alex Graves.\nGenerating sequences with recurrent neural networks.\narXiv preprint\narXiv:1308.0850, 2013.\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 770\u2013778, 2016.\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\u00fcrgen Schmidhuber. Gradient flow in\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\n[13] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation,\n9(8):1735\u20131780, 1997.\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\nLanguage Processing, pages 832\u2013841. ACL, August 2009.\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\n[16] \u0141ukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\nInformation Processing Systems, (NIPS), 2016.\n[17] \u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\non Learning Representations (ICLR), 2016.\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\n2017.\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\nIn International Conference on Learning Representations, 2017.\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\narXiv:1703.10722, 2017.\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\narXiv:1703.03130, 2017.\n[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\n11"
    },
    {
      "document": "doc2.pdf",
      "page": 7,
      "section_title": "length n is smaller than the representation dimensionality d",
      "importance_rank": 2,
      "refined_text": "length n is smaller than the representation dimensionality d, which is most often the case with\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\nthe input sequence centered around the respective output position. This would increase the maximum\npath length to O(n/r). We plan to investigate this approach further in future work.\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\nor O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths\nbetween any two positions in the network. Convolutional layers are generally more expensive than\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\nconsiderably, to O(k \u00b7 n \u00b7 d + n \u00b7 d2). Even with k = n, however, the complexity of a separable\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\nthe approach we take in our model.\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\nand semantic structure of the sentences.\n5\nTraining\nThis section describes the training regime for our models.\n5.1\nTraining Data and Batching\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\ntarget tokens.\n5.2\nHardware and Schedule\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\n(3.5 days).\n5.3\nOptimizer\nWe used the Adam optimizer [20] with \u03b21 = 0.9, \u03b22 = 0.98 and \u03f5 = 10\u22129. We varied the learning\nrate over the course of training, according to the formula:\nlrate = d\u22120.5\nmodel \u00b7 min(step_num\u22120.5, step_num \u00b7 warmup_steps\u22121.5)\n(3)\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\nwarmup_steps = 4000.\n5.4\nRegularization\nWe employ three types of regularization during training:\n7"
    },
    {
      "document": "doc2.pdf",
      "page": 12,
      "section_title": "[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice",
      "importance_rank": 3,
      "refined_text": "[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313\u2013330, 1993.\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\npages 152\u2013159. ACL, June 2006.\n[27] Ankur Parikh, Oscar T\u00e4ckstr\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\nmodel. In Empirical Methods in Natural Language Processing, 2016.\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433\u2013440. ACL, July\n2006.\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\npreprint arXiv:1608.05859, 2016.\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\nlayer. arXiv preprint arXiv:1701.06538, 2017.\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\nLearning Research, 15(1):1929\u20131958, 2014.\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\nAdvances in Neural Information Processing Systems 28, pages 2440\u20132448. Curran Associates,\nInc., 2015.\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\nnetworks. In Advances in Neural Information Processing Systems, pages 3104\u20133112, 2014.\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\nAdvances in Neural Information Processing Systems, 2015.\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\u2019s neural machine\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\narXiv:1609.08144, 2016.\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\n1: Long Papers), pages 434\u2013443. ACL, August 2013.\n12"
    },
    {
      "document": "doc2.pdf",
      "page": 2,
      "section_title": "1\nIntroduction\nRecurrent neural networks, long short-term me",
      "importance_rank": 4,
      "refined_text": "1\nIntroduction\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\nin particular, have been firmly established as state of the art approaches in sequence modeling and\ntransduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\narchitectures [38, 24, 15].\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht\u22121 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\ncomputation [32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\nare used in conjunction with a recurrent network.\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\n2\nBackground\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\nblock, computing hidden representations in parallel for all input and output positions. In these models,\nthe number of operations required to relate signals from two arbitrary input or output positions grows\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\naligned recurrence and have been shown to perform well on simple-language question answering and\nlanguage modeling tasks [34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirely on self-attention to compute representations of its input and output without using sequence-\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\nself-attention and discuss its advantages over models such as [17, 18] and [9].\n3\nModel Architecture\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\n[10], consuming the previously generated symbols as additional input when generating the next.\n2"
    },
    {
      "document": "doc2.pdf",
      "page": 10,
      "section_title": "Table 4: The Transformer generalizes well to English constit",
      "importance_rank": 5,
      "refined_text": "Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\nof WSJ)\nParser\nTraining\nWSJ 23 F1\nVinyals & Kaiser el al. (2014) [37]\nWSJ only, discriminative\n88.3\nPetrov et al. (2006) [29]\nWSJ only, discriminative\n90.4\nZhu et al. (2013) [40]\nWSJ only, discriminative\n90.4\nDyer et al. (2016) [8]\nWSJ only, discriminative\n91.7\nTransformer (4 layers)\nWSJ only, discriminative\n91.3\nZhu et al. (2013) [40]\nsemi-supervised\n91.3\nHuang & Harper (2009) [14]\nsemi-supervised\n91.3\nMcClosky et al. (2006) [26]\nsemi-supervised\n92.1\nVinyals & Kaiser el al. (2014) [37]\nsemi-supervised\n92.1\nTransformer (4 layers)\nsemi-supervised\n92.7\nLuong et al. (2015) [23]\nmulti-task\n93.0\nDyer et al. (2016) [8]\ngenerative\n93.3\nincreased the maximum output length to input length + 300. We used a beam size of 21 and \u03b1 = 0.3\nfor both WSJ only and the semi-supervised setting.\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\nprisingly well, yielding better results than all previously reported models with the exception of the\nRecurrent Neural Network Grammar [8].\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\nParser [29] even when training only on the WSJ training set of 40K sentences.\n7\nConclusion\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\nmulti-headed self-attention.\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\nmodel outperforms even all previously reported ensembles.\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\nplan to extend the Transformer to problems involving input and output modalities other than text and\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\nThe code we used to train and evaluate our models is available at https://github.com/\ntensorflow/tensor2tensor.\nAcknowledgements\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\ncomments, corrections and inspiration.\nReferences\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\nreading. arXiv preprint arXiv:1601.06733, 2016.\n10"
    }
  ]
}